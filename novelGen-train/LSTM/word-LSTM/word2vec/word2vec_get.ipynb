{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_get.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6X3Z7is0SAT"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "import jieba\n",
        "import os\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6v3D5NUJ0ay3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理语料：word2vec中需要sens是经过预处理且用空格进行分词的\n",
        "def readTxt(txtpath):\n",
        "  # txtpath=\"/content/drive/MyDrive/LSTM/dataG\"\n",
        "  txtfiles=os.listdir(txtpath)\n",
        "  txtsList=[]\n",
        "  for file in txtfiles:\n",
        "    position = txtpath + '/' + file\n",
        "    with open(position,\"r\",encoding='UTF-8') as f:   # 打开文件\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        if line!='\\n':\n",
        "          txtsList.append(line)  # 文本文件中每行为列表中一个元素\n",
        "  text = \"\"\n",
        "  for line in txtsList:\n",
        "    for tok in jieba.lcut(line):\n",
        "      text += tok\n",
        "      text += ' '\n",
        "  return text # 返回用空格分割的词语的文本字符串"
      ],
      "metadata": {
        "id": "pTjtdTTE0h0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取双向索引词典\n",
        "# {词：索引}字典\n",
        "def get_dict_tokenToIndex(data):\n",
        "  list_dict=Counter(data).most_common()\n",
        "  dict_tokenToIndex={}\n",
        "  i=0\n",
        "  for tup in list_dict:\n",
        "    key , value = tup\n",
        "    dict_tokenToIndex[key]=i\n",
        "    i+=1\n",
        "  # print(dict_token)\n",
        "  return dict_tokenToIndex\n",
        "\n",
        "# {索引：词}字典\n",
        "def get_dict_indexTotoken(dict_tokenToIndex):\n",
        "  dict_indexTotoken={}\n",
        "  for key , value in dict_tokenToIndex.items():\n",
        "    dict_indexTotoken[value] = key\n",
        "  return dict_indexTotoken"
      ],
      "metadata": {
        "id": "pbGANX7U4BDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processed1(txtpath,cutpath,tokToInt_path,intToTok_path,wordsnum_path):\n",
        "  texts=readTxt(txtpath)\n",
        "  with open(cutpath,'w') as f: # 存储用空格分割的词语的文本字符串到txt文件，便于之后word2vec.LineSentence的读取\n",
        "    f.write(texts) \n",
        "  texts=texts.split(' ')\n",
        "  # 获取词典\n",
        "  dict_tokToInt = get_dict_tokenToIndex(texts)\n",
        "  dict_intToTok = get_dict_indexTotoken(dict_tokToInt)\n",
        "  # 存储\n",
        "  np.save(tokToInt_path, dict_tokToInt)\n",
        "  np.save(intToTok_path, dict_intToTok)\n",
        "  np.save(wordsnum_path, len(dict_tokToInt))"
      ],
      "metadata": {
        "id": "8rqqQVKL2bKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(cutpath,wvsavepath):\n",
        "  # 读取分词后的文本文件\n",
        "  sentences=word2vec.LineSentence(cutpath)\n",
        "  # 训练模型，词向量的长度设置为128，迭代次数为10，0即CBOW模型,1采用skip-gram模型，模型保存为bin格式\n",
        "  model = gensim.models.Word2Vec(sentences, size=128, window=5 , min_count=1 , sg=0, iter=10)  \n",
        "  model.wv.save_word2vec_format(wvsavepath, binary=True)"
      ],
      "metadata": {
        "id": "5Fl9NPCI19yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getWordEmbedding(intToTok_path,wvsavepath,wordsweigth_path):\n",
        "  dict_intToTok = np.load(intToTok_path,allow_pickle=True).item()\n",
        "  wordVec = gensim.models.KeyedVectors.load_word2vec_format(wvsavepath, binary=True)\n",
        "  vocab = []\n",
        "  wordEmbedding = []\n",
        "  for index , word in dict_intToTok.items():\n",
        "    try:\n",
        "      vector = wordVec.wv[word]\n",
        "      vocab.append(index)\n",
        "      wordEmbedding.append(vector)\n",
        "    except:\n",
        "      print(str(index) + \":\" + word + \"不存在于词向量中\")\n",
        "      wordEmbedding.append([0]*128)\n",
        "      vocab.append(index)\n",
        "  print(\"词语数共：\",len(vocab))\n",
        "  np.save(wordsweigth_path,np.array(wordEmbedding))    # 词向量表示为num*dim \n",
        "  return np.array(wordEmbedding)"
      ],
      "metadata": {
        "id": "KmlKeRDl8cjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed1(txtpath,cutpath,tokToInt_path,intToTok_path,wordsnum_path)\n",
        "train_word2vec(cutpath,wvsavepath)"
      ],
      "metadata": {
        "id": "Nd1HG8ky-NBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding = getWordEmbedding(intToTok_path,wvsavepath,wordsweigth_path)"
      ],
      "metadata": {
        "id": "6nPHIXHLATT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding"
      ],
      "metadata": {
        "id": "ooQGdOdiHFcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}