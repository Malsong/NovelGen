{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fywq2AMLaX5J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 语料预处理"
      ],
      "metadata": {
        "id": "BSzJSN4aawYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loaddicts(wordsweight_path,tokToInt_path,intToTok_path):\n",
        "  # word2vec词向量获取、字典映射获取\n",
        "  wordEmbedding=np.load(wordsweight_path)\n",
        "  dict_tokToInt=np.load(tokToInt_path,allow_pickle=True).item()\n",
        "  dict_intToTok=np.load(intToTok_path,allow_pickle=True).item()\n",
        "  return wordEmbedding,dict_tokToInt,dict_intToTok"
      ],
      "metadata": {
        "id": "mzFQ3gF9bEa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized(cutpath,dict_tokToInt):\n",
        "  text_to_int = []\n",
        "  with open(cutpath,'r',encoding='UTF-8') as f:\n",
        "    texts=f.read()   # 获得分词后的以空格分割的文本字符串\n",
        "  texts=texts.split(' ')\n",
        "  for word in texts:\n",
        "    try:\n",
        "      text_to_int.append(dict_tokToInt[word]) # 将分词后的所有小说文本改成索引数字列表\n",
        "    except:\n",
        "      print(word+\"不在词典内\")\n",
        "  print(\"小说词数共\",len(text_to_int))\n",
        "  return text_to_int"
      ],
      "metadata": {
        "id": "zxL9vtlSbknT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建dataset存储输入目标对\n",
        "def get_pairs(text_to_int,seq_length,batch_size):\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(text_to_int) # 将索引转化为tensor\n",
        "  sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # 拆分文本索引序列\n",
        "  pairs_num = len(text_to_int) // (seq_length+1) # 全文所包含的所有输入目标对数\n",
        "  def split_input_target(sequence): # 此函数用于获得每个时间步的输入与目标序列\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "  # 将sequences拆分成输入目标对并随机打乱组成batch\n",
        "  datasets = sequences.map(split_input_target).shuffle(pairs_num).batch(batch_size, drop_remainder=True)#.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return datasets"
      ],
      "metadata": {
        "id": "bEWkHnazauVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建训练、验证、测试集\n",
        "def split_datasets(datasets):\n",
        "  # 8:1:1\n",
        "  dataset_size = len(datasets)\n",
        "  train_size = int(0.8 * dataset_size)\n",
        "  val_size = int(0.1 * dataset_size)\n",
        "  test_size = dataset_size - train_size - val_size\n",
        "  # print(train_size,val_size,test_size)\n",
        "  train_ds = datasets.take(train_size)\n",
        "  valid_test_ds = datasets.skip(train_size)\n",
        "  valid_ds = valid_test_ds.take(val_size)\n",
        "  test_ds = valid_test_ds.skip(val_size)\n",
        "  return train_ds,valid_ds,test_ds"
      ],
      "metadata": {
        "id": "V8_f40GGa8Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型创建"
      ],
      "metadata": {
        "id": "VBSPop6ad94T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "class MyModel_gui(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size, wordEmbedding):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,weights=[wordEmbedding],batch_input_shape=[batch_size,None])\n",
        "    self.LSTM = tf.keras.layers.LSTM(units=rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform')\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.LSTM.get_initial_state(x)\n",
        "    x = self.LSTM(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "wkmIMSJ8d_7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型训练"
      ],
      "metadata": {
        "id": "JexlBxRAeDuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 损失函数：计算模型预测值和真实值的差异\n",
        "def loss(y_true, y_pred):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "metadata": {
        "id": "bM00AFo1eDL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化模型，并进行训练\n",
        "def train(vocab_size,embedding_dim,rnn_units,batch_size,train_ds,valid_ds,epochs,learning_rate,wordEmbedding):\n",
        "  model = MyModel_gui(vocab_size=vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=batch_size,wordEmbedding=wordEmbedding)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss=loss)\n",
        "  history = model.fit(\n",
        "      train_ds,\n",
        "      epochs=epochs,\n",
        "      validation_data=valid_ds,\n",
        "      validation_freq=1\n",
        "  )\n",
        "  return model,history"
      ],
      "metadata": {
        "id": "Ln8SiuN-eGlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 执行训练"
      ],
      "metadata": {
        "id": "p9swXCwueJ8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed(cutpath,wordsweight_path,tokToInt_path,intToTok_path,text_to_int_path,seq_length,batch_size):\n",
        "  wordEmbedding,dict_tokToInt,dict_intToTok = loaddicts(wordsweight_path,tokToInt_path,intToTok_path)\n",
        "  text_to_int = tokenized(cutpath,dict_tokToInt)\n",
        "  # text_to_int存储\n",
        "  np.save(text_to_int_path,text_to_int)\n",
        "  # 获取全部datasets\n",
        "  datasets = get_pairs(text_to_int,seq_length,batch_size)\n",
        "  # 获取训练、验证、测试集\n",
        "  train_ds,valid_ds,test_ds= split_datasets(datasets)\n",
        "  return wordEmbedding,dict_tokToInt,dict_intToTok,train_ds,valid_ds,test_ds"
      ],
      "metadata": {
        "id": "BIH7cJVLeM-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 执行训练\n",
        "def main(cutpath,wordsweight_path,tokToInt_path,intToTok_path,text_to_int_path,seq_length,batch_size,embedding_dim,rnn_units,learning_rate,epochs):\n",
        "  wordEmbedding,dict_tokToInt,dict_intToTok,train_ds,valid_ds,test_ds = preprocessed(cutpath,wordsweight_path,tokToInt_path,intToTok_path,text_to_int_path,seq_length,batch_size)\n",
        "  vocab_size = len(dict_tokToInt)\n",
        "  model,history = train(vocab_size,embedding_dim,rnn_units,batch_size,train_ds,valid_ds,epochs,learning_rate,wordEmbedding)\n",
        "  model.save_weights('gui_best_weights.h5')\n",
        "  eval = model.evaluate(test_ds)\n",
        "  return history,eval"
      ],
      "metadata": {
        "id": "7c2TqebQa_HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history,eval = main(cutpath,wordsweight_path,tokToInt_path,intToTok_path,text_to_int_path,seq_length,batch_size,embedding_dim,rnn_units,learning_rate,epochs)"
      ],
      "metadata": {
        "id": "mmvRKghiiDrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 小说生成"
      ],
      "metadata": {
        "id": "xB_8qD4VmeY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba"
      ],
      "metadata": {
        "id": "aQdkYkacnLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 由下标索引转换成字，并连接成句\n",
        "def text_from_index(novel_ids,dicts_intToTok):\n",
        "  novel=\"\"\n",
        "  for id in novel_ids:\n",
        "    novel += dicts_intToTok[id]\n",
        "  return novel"
      ],
      "metadata": {
        "id": "Ui6HtJhXmj8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将句子转换成索引列表,text为字符串\n",
        "def index_form_text(text,dicts_tokToInt):\n",
        "  index=[]\n",
        "  for word in jieba.lcut(text):\n",
        "    index+=[dicts_tokToInt[word]]\n",
        "  return index"
      ],
      "metadata": {
        "id": "DFuLhkPUmkeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 专门用来生成的模型\n",
        "def generate_Model(embedding_dim,vocab_size,rnn_units,batch_size,ckpt_path):\n",
        "  gen_model = tf.keras.models.Sequential([\n",
        "    # 词嵌入层                                      \n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,\n",
        "                 batch_input_shape=[batch_size, None]),\n",
        "    # LSTM 层\n",
        "    tf.keras.layers.LSTM(units=rnn_units,return_sequences=True,stateful=True),\n",
        "\n",
        "    # 全连接层\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  ])\n",
        "  gen_model.load_weights(ckpt_path)      # 读入之前训练时存储下来的权重\n",
        "  gen_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  return gen_model"
      ],
      "metadata": {
        "id": "pg1T1bOxm4Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_para(path1,path2,path3):\n",
        "  dicts_intToTok = np.load(path1,allow_pickle=True).item()\n",
        "  dicts_tokToInt = np.load(path2,allow_pickle=True).item()\n",
        "  vocab_size = len(dicts_intToTok)\n",
        "  ckpt_path = path3\n",
        "  return dicts_intToTok,dicts_tokToInt,vocab_size,ckpt_path"
      ],
      "metadata": {
        "id": "7moW7_Xsmy5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_novel(model,start_text,words_num,dicts_ci,temperature):\n",
        "  start_index = index_form_text(start_text,dicts_ci)\n",
        "  generateText = []\n",
        "  for i in range(words_num):\n",
        "    if i < len(start_index):\n",
        "      generateText+=[start_index[i]]\n",
        "    input = tf.expand_dims([generateText[i]], axis=0)\n",
        "    predictions = model(input)\n",
        "    \n",
        "    predictions = tf.squeeze(predictions, 0)   #这个张量是将原始input中所有维度为1的那些维都删掉的结果\n",
        "    predictions /= temperature\n",
        "    \n",
        "\n",
        "    # 从一个分类分布中抽取样本(;num_samples:抽取的样本个数)  #\n",
        "    # logits:形状为 [batch_size, num_classes]的张量. 每个切片[i, :]代表对于所有类的未正规化的log概率。\n",
        "    # 最后softmax的概率分布；也可以是整数，会自动变换成概率分布\n",
        "    sampled_indices = tf.random.categorical(predictions, num_samples=1)  \n",
        "\n",
        "    if i >= len(start_index)-1:\n",
        "      generateText += list(sampled_indices.numpy()[0])\n",
        "  return generateText"
      ],
      "metadata": {
        "id": "toy9dn4joCcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(start_words,temperature,gen_num):\n",
        "  embedding_dim = 128\n",
        "  rnn_units = 1024\n",
        "  batch_size = 1\n",
        "  # start_words=\"我的话刚说了一半，便听一声巨响，顶门的木椅突然被撞成了数断，\"\n",
        "  path1='/content/drive/MyDrive/LSTM/word-LSTM/word2vec/npy_etc/gui_dict_intToTok.npy'\n",
        "  path2='/content/drive/MyDrive/LSTM/word-LSTM/word2vec/npy_etc/gui_dict_tokToInt.npy'\n",
        "  path3='/content/drive/MyDrive/LSTM/word-LSTM/word2vec/model_save/gui_best_weights.h5'\n",
        "  dicts_ic,dicts_ci,vocab_size,ckpt_path = load_para(path1,path2,path3)\n",
        "  model_gen = generate_Model(embedding_dim,vocab_size,rnn_units,batch_size,ckpt_path)\n",
        "  generateText = gen_novel(model_gen,start_words,gen_num,dicts_ci,temperature)\n",
        "  finalText = text_from_index(generateText,dicts_ic)\n",
        "  print(finalText)"
      ],
      "metadata": {
        "id": "_5QNfIUkoC51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_words=\"我的话刚说了一半，便听一声巨响，顶门的木椅突然被撞成了数断，\"\n",
        "temperature = 0.8\n",
        "gen_num = 800"
      ],
      "metadata": {
        "id": "91DqN2Lso4LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print('*'*30)\n",
        "  generate(start_words,temperature,gen_num)"
      ],
      "metadata": {
        "id": "yQXKufOlUMAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}