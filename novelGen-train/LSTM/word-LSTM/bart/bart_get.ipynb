{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bart_wwget.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OczvfMUqyoCP"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer, BartForConditionalGeneration\n",
        "import collections\n",
        "from collections import Counter\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TxLcFZmhy7CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 语料已清洗，对原始语料进行读取\n",
        "def readtxt(txtspath):\n",
        "  txtsList=[]\n",
        "  txtfiles=os.listdir(txtspath)\n",
        "  for file in txtfiles:\n",
        "    position = txtspath + '/' + file\n",
        "    # print(position)\n",
        "    with open(position,\"r\",encoding='UTF-8') as f:   # 打开文件\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        if line!='\\n':\n",
        "          # line = line.strip()+'[SEP]'\n",
        "          txtsList.append(line)\n",
        "  text = \"\"\n",
        "  for line in txtsList:\n",
        "    text += line\n",
        "  return text # 返回语料段落列表"
      ],
      "metadata": {
        "id": "5Scudi6HzZV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载字典\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab_dicts = {}\n",
        "  with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "    tokens = reader.readlines()\n",
        "  for index, token in enumerate(tokens):\n",
        "    token = token.rstrip(\"\\n\")\n",
        "    vocab_dicts[token] = index\n",
        "  print('len(vocab_dicts):',len(vocab_dicts))\n",
        "  return vocab_dicts"
      ],
      "metadata": {
        "id": "n3MpycZnzmx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取bart全部21128字向量\n",
        "def get_wholeEmbed(model):\n",
        "  embed_weight=model.state_dict()[\"model.encoder.embed_tokens.weight\"]\n",
        "  print(embed_weight.shape)\n",
        "  np.save('wholebart_charsweight.npy', embed_weight)\n",
        "  return embed_weight"
      ],
      "metadata": {
        "id": "2wFSqsnY1rUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_para(intToTok_path,tokToInt_path):\n",
        "  # 文本的jieba词语字典\n",
        "  dict_intToTok = np.load(intToTok_path,allow_pickle=True).item()\n",
        "  dict_TokToInt = np.load(tokToInt_path,allow_pickle=True).item()\n",
        "  return dict_intToTok,dict_TokToInt"
      ],
      "metadata": {
        "id": "VC7Wvtx76_Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将分词的词语用berttokenizer分词，然后从字向量中遍历取得各向量并求平均\n",
        "def get_bart_wordsweight(tokenizer,dict_intToTok,vocab_dicts,wordEmbedding):\n",
        "  wordsweight=[]\n",
        "  for num,tok in dict_intToTok.items():  # 结巴分词后的词语\n",
        "    tok_charList = tokenizer.tokenize(tok)   # 分成字符\n",
        "    tokweight=[]\n",
        "    for char in tok_charList:\n",
        "      try:\n",
        "        index = vocab_dicts[char]\n",
        "        tokweight.append(wordEmbedding[index]) \n",
        "        #print(charsweight[index])\n",
        "      except:\n",
        "        print(char+\"不在字典中\")\n",
        "        if char == '\\n':\n",
        "          tokweight.append(wordEmbedding[vocab_dicts['[SEP]']]) \n",
        "    total = [0]*768\n",
        "    if tokweight==[]:\n",
        "      tokweight.append(np.array([0]*768))\n",
        "    for i in range(len(tokweight)):\n",
        "      total += np.array(tokweight[i])\n",
        "    wordsweight.append( total / len(tokweight) )\n",
        "  return np.array(wordsweight)"
      ],
      "metadata": {
        "id": "IDL1AnNQ6vOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_intToTok,dict_TokToInt = load_para(intToTok_path,tokToInt_path)"
      ],
      "metadata": {
        "id": "LV41C6v347sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer(vocab_file=vocab_path)\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"fnlp/bart-base-chinese\")"
      ],
      "metadata": {
        "id": "LAPqaM9qzB_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding = get_wholeEmbed(model_bart) # 获取bart全部21128字向量\n",
        "vocab_dicts = load_vocab(vocab_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KWa4E2L6WO_",
        "outputId": "0e54ab4d-73ea-4614-a893-d2ff9f35c2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21128, 768])\n",
            "len(vocab_dicts): 21128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding = get_bart_wordsweight(tokenizer,dict_intToTok,vocab_dicts,wordEmbedding)"
      ],
      "metadata": {
        "id": "TfTtQtwx4n6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}