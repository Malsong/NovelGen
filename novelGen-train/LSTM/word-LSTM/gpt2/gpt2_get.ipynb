{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_ww.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4kxRw1mpHH5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import GPT2LMHeadModel,BertTokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1orsh0g1pWGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载字典\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab_dicts = {}\n",
        "  with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "    tokens = reader.readlines()\n",
        "  for index, token in enumerate(tokens):\n",
        "    token = token.rstrip(\"\\n\")\n",
        "    vocab_dicts[token] = index\n",
        "  print('len(vocab_dicts):',len(vocab_dicts))\n",
        "  return vocab_dicts"
      ],
      "metadata": {
        "id": "groK0wmvp6cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_para(intToTok_path,tokToInt_path):\n",
        "  # 文本的jieba词语字典\n",
        "  dict_intToTok = np.load(intToTok_path,allow_pickle=True).item()\n",
        "  dict_TokToInt = np.load(tokToInt_path,allow_pickle=True).item()\n",
        "  return dict_intToTok,dict_TokToInt"
      ],
      "metadata": {
        "id": "sArC9CT1pfHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将分词的词语用berttokenizer分词，然后从字向量中遍历取得各向量并求平均\n",
        "def get_gpt2_wordsweight(tokenizer,dict_intToTok,vocab_dicts,wholeEmbedding):\n",
        "  wordsweight=[]\n",
        "  for num,tok in dict_intToTok.items():  # 结巴分词后的词语\n",
        "    tok_charList = tokenizer.tokenize(tok)   # 分成字符\n",
        "    tokweight=[]\n",
        "    for char in tok_charList:\n",
        "      try:\n",
        "        index = vocab_dicts[char]\n",
        "        tokweight.append(wholeEmbedding[index]) \n",
        "        #print(charsweight[index])\n",
        "      except:\n",
        "        print(char+\"不在字典中\")\n",
        "        if char == '\\n':\n",
        "          tokweight.append(wholeEmbedding[vocab_dicts['[SEP]']]) \n",
        "    total = [0]*768\n",
        "    if tokweight==[]:\n",
        "      tokweight.append(np.array([0]*768))\n",
        "    for i in range(len(tokweight)):\n",
        "      total += np.array(tokweight[i])\n",
        "    wordsweight.append( total / len(tokweight) )\n",
        "  return np.array(wordsweight)"
      ],
      "metadata": {
        "id": "fjxEhCHMp_TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取gpt2全部21128字向量\n",
        "def get_wholeEmbed(model):\n",
        "  embed_weight=model.transformer.wte.weight # Word Token Embeddings\n",
        "  print(embed_weight.shape)\n",
        "  np.save('wholebart_charsweight.npy', embed_weight.detach().numpy())\n",
        "  return embed_weight.detach().numpy()"
      ],
      "metadata": {
        "id": "sElZi4fXqHiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_intToTok,dict_TokToInt = load_para(intToTok_path,tokToInt_path)"
      ],
      "metadata": {
        "id": "l22I2DYZrxUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer(vocab_file=vocab_path)\n",
        "model_gpt2 = GPT2LMHeadModel.from_pretrained(filename)"
      ],
      "metadata": {
        "id": "saAuqOGdrenI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wholeEmbedding = get_wholeEmbed(model_gpt2) # 获取bart全部21128字向量\n",
        "vocab_dicts = load_vocab(vocab_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGH-KyWnr1oX",
        "outputId": "7f5492a9-b638-4293-c07e-93c72895ef4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21128, 768])\n",
            "len(vocab_dicts): 21128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding = get_gpt2_wordsweight(tokenizer,dict_intToTok,vocab_dicts,wholeEmbedding)"
      ],
      "metadata": {
        "id": "HpjKPIHTr5Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordEmbedding"
      ],
      "metadata": {
        "id": "YKrbOW3zt-M3",
        "outputId": "1dc542c0-8839-4927-dd51-3c15428bb76e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.03963371,  0.02159631, -0.01067799, ...,  0.02621134,\n",
              "         0.02223622, -0.01290673],\n",
              "       [ 0.0731354 ,  0.01905653, -0.03433763, ...,  0.00874878,\n",
              "        -0.01870194,  0.02161469],\n",
              "       [ 0.08375222,  0.03868634, -0.0608225 , ...,  0.10359563,\n",
              "         0.02695078, -0.00318346],\n",
              "       ...,\n",
              "       [-0.01315529,  0.04295164, -0.04904548, ..., -0.00890463,\n",
              "         0.01013792, -0.04075794],\n",
              "       [ 0.02195235,  0.04152397, -0.0969019 , ...,  0.00263877,\n",
              "        -0.01511827,  0.05970754],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}