{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhVvB9y8leVo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 小说语料预处理"
      ],
      "metadata": {
        "id": "KnDrwWpzlfSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 语料已清洗，对语料进行读取\n",
        "def readtxt(txtspath):\n",
        "  txtsList=[]\n",
        "  txtfiles=os.listdir(txtspath)\n",
        "  for file in txtfiles:\n",
        "    position = txtspath + '/' + file\n",
        "    # print(position)\n",
        "    with open(position,\"r\",encoding='UTF-8') as f:   # 打开文件\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        if line!='\\n':\n",
        "          txtsList.append(line)\n",
        "  return txtsList # 返回语料段落列表"
      ],
      "metadata": {
        "id": "bKSSDeTrliW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 对语料进行索引化\n",
        "def tokenized(txtsList):\n",
        "  text = \"\"\n",
        "  for line in txtsList:\n",
        "    text += line # 读取为段落后带换行符的 # print(text[:1000])\n",
        "  total_num = len(text)\n",
        "  unique_num = len(set(text))\n",
        "  print(f'全文共 {total_num} 个汉字')     # 文本总字数\n",
        "  print(f'其中不重复出现汉字为 {unique_num} 个')  # 不重复字符数\n",
        "  # 创建以字为单位的Tokenizer\n",
        "  num_chars = unique_num\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_chars,char_level=True,filters='')\n",
        "  tokenizer.fit_on_texts(text) # 读小说文本\n",
        "  text_to_int = tokenizer.texts_to_sequences([text])[0] # 将字加入字典并转化索引数字列表\n",
        "  # 创建双向索引字典\n",
        "  dicts_ic = tokenizer.index_word # {索引下标：字}\n",
        "  dicts_ci = {}\n",
        "  for key,value in dicts_ic.items():\n",
        "    dicts_ci[value] = key\n",
        "  return num_chars,tokenizer,text_to_int,dicts_ic,dicts_ci"
      ],
      "metadata": {
        "id": "HbEgxsM-n7mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建dataset存储输入目标对\n",
        "def get_pairs(text_to_int,seq_length,batch_size):\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(text_to_int) # 将索引转化为tensor\n",
        "  sequences = ids_dataset.batch(seq_length+1, drop_remainder=True) # 拆分文本索引序列\n",
        "  pairs_num = len(text_to_int) // (seq_length+1) # 全文所包含的所有输入目标对数\n",
        "  def split_input_target(sequence): # 此函数用于获得每个时间步的输入与目标序列\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "  # 将sequences拆分成输入目标对并随机打乱组成batch\n",
        "  datasets = sequences.map(split_input_target).shuffle(pairs_num).batch(batch_size, drop_remainder=True)#.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return datasets"
      ],
      "metadata": {
        "id": "AwOvytNbu0oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建训练、验证、测试集\n",
        "def split_datasets(datasets):\n",
        "  # 8:1:1\n",
        "  dataset_size = len(datasets)\n",
        "  train_size = int(0.8 * dataset_size)\n",
        "  val_size = int(0.1 * dataset_size)\n",
        "  test_size = dataset_size - train_size - val_size\n",
        "  # print(train_size,val_size,test_size)\n",
        "  train_ds = datasets.take(train_size)\n",
        "  valid_test_ds = datasets.skip(train_size)\n",
        "  valid_ds = valid_test_ds.take(val_size)\n",
        "  test_ds = valid_test_ds.skip(val_size)\n",
        "  return train_ds,valid_ds,test_ds"
      ],
      "metadata": {
        "id": "ZPLjqcNa1KKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型创建"
      ],
      "metadata": {
        "id": "iumIvcP3zLv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "class MyModel_gui(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,batch_input_shape=[batch_size,None])\n",
        "    self.LSTM = tf.keras.layers.LSTM(units=rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform')\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.LSTM.get_initial_state(x)\n",
        "    x = self.LSTM(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YSNAgAfM01KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型训练"
      ],
      "metadata": {
        "id": "yHLRoIpG1PMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 损失函数：计算模型预测值和真实值的差异\n",
        "def loss(y_true, y_pred):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "metadata": {
        "id": "hvjxUTQ61wnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化模型，并进行训练\n",
        "def train(vocab_size,embedding_dim,rnn_units,batch_size,train_ds,valid_ds,epochs,learning_rate):\n",
        "  model = MyModel_gui(vocab_size=vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=batch_size)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),loss=loss)\n",
        "  history = model.fit(\n",
        "      train_ds,\n",
        "      epochs=epochs,\n",
        "      validation_data=valid_ds,\n",
        "      validation_freq=1\n",
        "  )\n",
        "  return model,history"
      ],
      "metadata": {
        "id": "on0pGT6T2umM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 执行训练"
      ],
      "metadata": {
        "id": "dZyieTnY4ULX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed(txtspath,seq_length,batch_size):\n",
        "  txtsList = readtxt(txtspath)\n",
        "  num_chars,tokenizer,text_to_int,dicts_ic,dicts_ci = tokenized(txtsList)\n",
        "  # 存储双向索引字典\n",
        "  np.save('/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_dict_ic.npy', dicts_ic)\n",
        "  np.save('/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_dicts_ci.npy', dicts_ci)\n",
        "  np.save('/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_charsnum.npy', num_chars)\n",
        "  # 获取全部datasets\n",
        "  datasets = get_pairs(text_to_int,seq_length,batch_size)\n",
        "  # 获取训练、验证、测试集\n",
        "  train_ds,valid_ds,test_ds= split_datasets(datasets)\n",
        "  return num_chars,train_ds,valid_ds,test_ds,datasets"
      ],
      "metadata": {
        "id": "PNLaAyDu6aEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 执行训练\n",
        "def main(txtspath,seq_length,batch_size,embedding_dim,rnn_units,learning_rate,epochs):\n",
        "  vocab_size,train_ds,valid_ds,test_ds,datasets = preprocessed(txtspath,seq_length,batch_size)\n",
        "  model,history = train(vocab_size,embedding_dim,rnn_units,batch_size,train_ds,valid_ds,epochs,learning_rate)\n",
        "  model.save_weights('/content/drive/MyDrive/LSTM/char-LSTM/model_save/gui_best_weights.h5')\n",
        "  eval = model.evaluate(test_ds)\n",
        "  return history,eval"
      ],
      "metadata": {
        "id": "zAO1Vnyq0HsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练参数设置\n",
        "txtspath = '/content/drive/MyDrive/LSTM/dataG'\n",
        "seq_length = 50\n",
        "batch_size = 64\n",
        "embedding_dim = 512\n",
        "rnn_units = 1024\n",
        "learning_rate = 0.00125\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "tLarXnWdLzaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history,eval = main(txtspath,seq_length,batch_size,embedding_dim,rnn_units,learning_rate,epochs)"
      ],
      "metadata": {
        "id": "Sp_8mG2bMJDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 小说生成"
      ],
      "metadata": {
        "id": "dTAAvTBazNhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 将句子转换成索引列表,text为字符串\n",
        "def index_form_text(text,dicts_ci):\n",
        "  index=[]\n",
        "  for char in text:\n",
        "    index+=[dicts_ci[char]]\n",
        "  return index"
      ],
      "metadata": {
        "id": "R4mxS0cvNuMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 由下标索引转换成字，并连接成句\n",
        "def text_from_index(novel_ids,dicts_ic):\n",
        "  novel=\"\"\n",
        "  for id in novel_ids:\n",
        "    novel += dicts_ic[id]\n",
        "  return novel"
      ],
      "metadata": {
        "id": "a7GTwVwUzPfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 专门用来生成的模型\n",
        "def generate_Model(embedding_dim,vocab_size,rnn_units,batch_size,ckpt_path):\n",
        "  gen_model = tf.keras.models.Sequential([\n",
        "    # 词嵌入层                                      \n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,\n",
        "                 batch_input_shape=[batch_size, None]),\n",
        "    # LSTM 层\n",
        "    tf.keras.layers.LSTM(units=rnn_units,return_sequences=True,stateful=True),\n",
        "\n",
        "    # 全连接层\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  ])\n",
        "  gen_model.load_weights(ckpt_path)      # 读入之前训练时存储下来的权重\n",
        "  gen_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "  return gen_model"
      ],
      "metadata": {
        "id": "1-6nqC_SpPoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_para(path1,path2,path3,path4):\n",
        "  dicts_ic = np.load(path1,allow_pickle=True).item()\n",
        "  dicts_ci = np.load(path2,allow_pickle=True).item()\n",
        "  vocab_size = np.load(path3,allow_pickle=True).item()\n",
        "  ckpt_path = path4\n",
        "  return dicts_ic,dicts_ci,vocab_size,ckpt_path"
      ],
      "metadata": {
        "id": "hAJD7Aq9pXVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_novel(model,start_text,words_num,dicts_ci,temperature):\n",
        "  start_index = index_form_text(start_text,dicts_ci)\n",
        "  generateText = []\n",
        "  for i in range(words_num):\n",
        "    if i < len(start_index):\n",
        "      generateText+=[start_index[i]]\n",
        "    input = tf.expand_dims([generateText[i]], axis=0)\n",
        "    predictions = model(input)\n",
        "    \n",
        "    predictions = tf.squeeze(predictions, 0)   #这个张量是将原始input中所有维度为1的那些维都删掉的结果\n",
        "    predictions /= temperature\n",
        "    \n",
        "\n",
        "    # 从一个分类分布中抽取样本(;num_samples:抽取的样本个数)  #\n",
        "    # logits:形状为 [batch_size, num_classes]的张量. 每个切片[i, :]代表对于所有类的未正规化的log概率。\n",
        "    # 最后softmax的概率分布；也可以是整数，会自动变换成概率分布\n",
        "    sampled_indices = tf.random.categorical(predictions, num_samples=1)  \n",
        "\n",
        "    if i >= len(start_index)-1:\n",
        "      generateText += list(sampled_indices.numpy()[0])\n",
        "  return generateText"
      ],
      "metadata": {
        "id": "xwOqzw2iqE6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(start_words,temperature,gen_num):\n",
        "  embedding_dim = 512\n",
        "  rnn_units = 1024\n",
        "  batch_size = 1\n",
        "  # start_words=\"我的话刚说了一半，便听一声巨响，顶门的木椅突然被撞成了数断，\"\n",
        "  path1='/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_dict_ic.npy'\n",
        "  path2='/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_dict_ci.npy'\n",
        "  path3='/content/drive/MyDrive/LSTM/char-LSTM/npy/gui_charsnum.npy'\n",
        "  path4='/content/drive/MyDrive/LSTM/char-LSTM/model_save/gui_best_weights.h5'\n",
        "  dicts_ic,dicts_ci,vocab_size,ckpt_path = load_para(path1,path2,path3,path4)\n",
        "  model_gen = generate_Model(embedding_dim,vocab_size,rnn_units,batch_size,ckpt_path)\n",
        "  generateText = gen_novel(model_gen,start_words,gen_num,dicts_ci,temperature)\n",
        "  finalText = text_from_index(generateText,dicts_ic)\n",
        "  print(finalText)"
      ],
      "metadata": {
        "id": "tKXVyM3HpdHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_words=\"我的话刚说了一半，便听一声巨响，顶门的木椅突然被撞成了数断，\"\n",
        "temperature = 0.8\n",
        "gen_num = 800"
      ],
      "metadata": {
        "id": "5Ws3kI2trH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print('*'*30)\n",
        "  generate(start_words,temperature,gen_num)"
      ],
      "metadata": {
        "id": "g5yNck7H8gli"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}